import json
import os
import boto3
from botocore.exceptions import ClientError

try:
    ## Works when running in lambda:
    from utilsparam import s3 as utilsparams3
    from utilsparam import ssm as utilsparamssm
    from utilsparam import ec2 as utilsparamec2
    from utilsparam import events as utilsparamevents
    from utilsparam import pricing as utilsparampricing
except Exception as e:
    try:
        ## Most likely this comes from pytest and relative imports. 
        from ncap_iac.protocols.utilsparam import s3 as utilsparams3
        from ncap_iac.protocols.utilsparam import ssm as utilsparamssm
        from ncap_iac.protocols.utilsparam import ec2 as utilsparamec2
        from ncap_iac.protocols.utilsparam import events as utilsparamevents
        from ncap_iac.protocols.utilsparam import pricing as utilsparampricing
    except Exception as e_supp:
        error = str(e)+str(e_supp)
        stacktrace = json.dumps(traceback.format_exc())
        message = "Exception: " + error + "  Stacktrace: " + stacktrace
        err = {"message": message}
        print(err)
        raise

s3_resource = utilsparams3.s3_resource
s3_client = utilsparams3.s3_client

class PostProcess():
    """Generic class to handle postprocessing. Input is the path to the end.txt file.

    :param bucket: (str) name of bucket where endfile is located 
    :param endfile: (str) end.txt file given to initialize processing. 
    :param targetbucket: (str) name of bucket where we should write the next step.  
    :param stepname: (str) name to associate with postprocessing, i.e. ("step2"). should be unique in the workflow.
    """
    def __init__(self,bucket,endfile,targetbucket,stepname):
        self.bucket = bucket
        self.endfile = endfile
        ## do some path parsing: 
        self.jobdir = os.path.dirname(os.path.dirname(self.endfile))
        self.groupdir = os.path.dirname(os.path.dirname(self.jobdir))
        self.targetbucket = targetbucket
        self.stepname = stepname

    def get_timestamp(self):    
        """Get the timestamp identifier associated with this job from the endfile location. Will be used to initialize new submit files. 
        Note: may not be the exact timestamp due to path formatting conventions, but will match the job folder, which is all we need. 

        """
        job_folder = os.path.basename(os.path.normpath(self.jobdir))
        timestamp = job_folder.split("job__{b}_".format(b = self.bucket))[-1]
        return timestamp

    def get_endfile(self):   
        """Get the endfile. Just useful to know it exists. 

        """
        try:
            file_object = s3_resource.Object(self.bucket, self.endfile)
            raw_content = file_object.get()['Body'].read().decode('utf-8')
        except Exception as e:    
            print(e)

            raise Exception("file s3://{b}/{k} was not found".format(b=self.bucket, k=self.endfile))
        return raw_content

    def check_postprocess(self):
        """Check if postprocessing has already been started in this directory by looking for a file named filename in the same directory as self.endfile 
        
        :param filename: name of the file to look for in os.path.dirname(self.endfile)
        :returns: (boolean) gives true if the file exists, false if not.  
        """
        try:
            enddir = os.path.dirname(self.endfile)
            postfile = os.path.join(enddir,self.stepname)
            file_object = s3_resource.Object(self.bucket,postfile)
            file_object.load()
            postprocessed = True
        except ClientError as e:    
            if e.response["Error"]["Code"] == "404":
                print("Not Found")
                postprocessed = False
            else:    
                raise ClientError
        return postprocessed    

    def write_postprocess(self,body = ""):
        """Write a file named filename in the same directory as self.endfile  
        
        :param body: (str) content to write to the file, if any.  
        :returns: (boolean) gives true if the file exists, false if not.  
        """
        enddir = os.path.dirname(self.endfile)
        s3_resource.Bucket(self.bucket).put_object(
                Key = os.path.join(enddir,self.stepname),
                Body = bytes(body.encode('UTF-8'))
                )
    
    def copy_logs(self):
        """Copy logs generated by the first step (results/job__{bucketname}_{timestamp}/logs) into a folder (results/job__{bucketname}_{timestamp}/logs_pre_{stepname}) in the target bucket. 

        """
        ## 
        lognames = utilsparams3.ls_name(self.bucket,os.path.join(self.jobdir,"logs"))
        lognames_base = [os.path.basename(l) for l in lognames]
        ## now create keys:
        lognames_full = [os.path.join(self.jobdir,"logs_pre_{}".format(self.stepname),lb) for lb in lognames_base]
        for ln,lf in zip(lognames,lognames_full):
            s3_resource.meta.client.copy({"Bucket":self.bucket,"Key":ln},self.targetbucket,lf)

    def create_submitfile(self,datapaths,configpath):
        """Given a path to a dataset/datasets, as well as a configurationpath, (importantly both in the self.target bucket) creates a submitfile that can be used to trigger the postprocessing job. 

        :param datapaths: a list of keys within the target bucket to use as datapaths. 
        :param configpath: a list of configuration paths. 
        :returns: a dictionary that can be dumped into a json submit file. 
        """
        return {"dataname":datapaths,"configname":configpath,"timestamp":self.get_timestamp()}

    def submit(self,submitdict):
        """Submit a given submitfile to the target bucket for processing.  
        :param submitdict: the submission dictionary you want to use. 

        """
        utilsparams3.put_json(self.targetbucket,os.path.join(self.groupdir,"submissions","{}_submit.json".format(self.stepname)),submitdict)

class PostProcess_EnsembleDGPPredict(PostProcess):
    """Postprocessing for ensemble dgp. After all models in the ensemble have been trained, triggers the next step, prediction. Writes a config file that will work for prediction, and compiles a list of all videos we want to process next. . 

    """
    def load_config(self):
        """Load in an existing config file from the results area to help us fill in a new one. 
        We assume that the config files in question will be called inst{modelindex}config.json.
        If the ensemble is successfully trained, we should expect at most max(ensemble_size,9) models. 
        """
        ## assume that ensemble is of size >=1, so we get inst1config.json
        jobdirname = os.path.basename(os.path.normpath(self.jobdir))
        configpath = os.path.join(self.jobdir,"process_results","{}inst1config.json".format(jobdirname)) 
        configdict = utilsparams3.load_json(self.bucket,configpath)
        return configdict
        
    def make_config(self):
        """Write a config file for the prediction step. 

        :returns: a dictionary that can be dumped to a config file. 
        """
        ## initialize from inst1config:
        configdict = self.load_config()
        ensemblesize = max(configdict["ensemble_size"],9)
        configdict["mode"] = "predict"
        configdict["modelpath"] = os.path.join(self.jobdir,"process_results/").split("/",1)[-1]
        modelnames = ["ensemble-model{}-2030-01-0{}".format(str(i+1),str(i+1)) for i in range(ensemblesize)]
        configdict["modelnames"] = modelnames
        return configdict

    def write_config(self,configdict):
        """Submit a given submitfile to the target bucket for processing.  
        :param configdict: the config dictionary you want to use. 
        :returns configpath: path to config file

        """
        configpath = os.path.join(self.jobdir,"process_results","{}_autoconfig.json".format(self.stepname))
        utilsparams3.put_json(self.targetbucket,configpath,configdict)
        return configpath

    def get_videos(self):    
        """Get a list of videos included in the paths here: given under "modelname/videos". 

        """
        configdict = self.load_config()
        ensemblesize = max(configdict["ensemble_size"],9)
        ## video list: assumed identical bc we copied all from the same.  
        vidlist = utilsparams3.ls_name(self.bucket,os.path.join(self.jobdir,"process_results","ensemble-model1-2030-01-01","videos/")) ## this is assumed to be a fixed output of dgp models. 
        return vidlist

def postprocess_prediction_run(bucket_name,key):
    """Full packaging of all steps necessary to make postprocessing happen.
    """
    pp = PostProcess_EnsembleDGPPredict(bucket_name,key,bucket_name,"prediction")
    assert not pp.check_postprocess(), "Will only run postprocessing if not already marked as having postprocessed. "
    pp.copy_logs()
    config = pp.make_config()
    configpath = pp.write_config(config)
    videos = pp.get_videos()
    submit = pp.create_submitfile(datapaths=videos,configpath=configpath)
    pp.submit(submit)
    pp.write_postprocess(body = "starting prediction")
    return pp

def postprocess_prediction(event, context):
    """Postprocessing to create another job in the same bucket

    """
    
    for record in event['Records']:
        time = record['eventTime']
        bucket_name = record['s3']['bucket']['name']
        key = record['s3']['object']['key']
        
    ## Declare the bucket 
    postprocess_prediction_run(bucket_name,key)
